import numpy as np
import math as math
from pylab import * 
import py.ldpc as ldpc
import matplotlib.pyplot as plt
import time
import csv
from bitarray import bitarray


# Code taken from Adam's python tutorial. 
# I have removed the power allocation code as I'm using a uniform power allocation
# Can get this from the notebook if I decide I need this. 


try:
    from pyfht import fht_inplace
except ImportError:
    import warnings
    warnings.warn("Using very slow Python version of fht, please install pyfht")
    def fht_inplace(x):
        N = len(x)
        i = N>>1
        while i:
            for k in range(0, N, 2*i):
                for j in range(k, i+k):
                    ij = i|j
                    temp = x[j]
                    x[j] += x[ij]
                    x[ij] = temp - x[ij]
            i = i >> 1


def sub_fht(n, m, seed=0, ordering=None):
    """
    Returns functions to compute the sub-sampled Walsh-Hadamard transform,
    i.e., operating with a wide rectangular matrix of random +/-1 entries.

    n: number of rows
    m: number of columns

    It is most efficient (but not required) for max(m+1,n+1) to be a power of 2.

    seed: determines choice of random matrix
    ordering: optional n-long array of row indices in [1, max(m,n)] to
              implement subsampling; generated by seed if not specified,
              but may be given to speed up subsequent runs on the same matrix.

    Returns (Ax, Ay, ordering):
        Ax(x): computes A.x (of length n), with x having length m
        Ay(y): computes A'.y (of length m), with y having length n
        ordering: the ordering in use, which may have been generated from seed
    """
    assert n > 0, "n must be positive"
    assert m > 0, "m must be positive"
    w = 2**int(np.ceil(np.log2(max(m+1, n+1))))

    if ordering is not None:
        assert ordering.shape == (n,)
    else:
        rng = np.random.RandomState(seed)
        idxs = np.arange(1, w, dtype=np.uint32)
        rng.shuffle(idxs)
        ordering = idxs[:n]

    def Ax(x):
        assert x.size == m, "x must be m long"
        y = np.zeros(w)
        y[w-m:] = x.reshape(m)
        fht_inplace(y)
        return y[ordering]

    def Ay(y):
        assert y.size == n, "input must be n long"
        x = np.zeros(w)
        x.flat[ordering] = y
        fht_inplace(x)
        return x[w-m:]

    return Ax, Ay, ordering

def block_sub_fht(n, m, l, seed=0, ordering=None):
    """
    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially
    offering substantial speed improvements.

    n: number of rows
    m: number of columns per block
    l: number of blocks

    It is most efficient (though not required) when max(m+1,n+1) is a power of 2.

    seed: determines choice of random matrix
    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to
              implement subsampling; generated by seed if not specified, but
              may be given to speed up subsequent runs on the same matrix.

    Returns (Ax, Ay, ordering):
        Ax(x): computes A.x (of length n), with x having length l*m
        Ay(y): computes A'.y (of length l*m), with y having length n
        ordering: the ordering in use, which may have been generated from seed
    """
    assert n > 0, "n must be positive"
    assert m > 0, "m must be positive"
    assert l > 0, "l must be positive"

    if ordering is not None:
        assert ordering.shape == (l, n)
    else:
        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))
        rng = np.random.RandomState(seed)
        ordering = np.empty((l, n), dtype=np.uint32)
        idxs = np.arange(1, w, dtype=np.uint32)
        for ll in range(l):
            rng.shuffle(idxs)
            ordering[ll] = idxs[:n]
        

    def Ax(x):
        assert x.size == l*m
        out = np.zeros(n)
        for ll in range(l):
            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])
            out += ax(x[ll*m:(ll+1)*m])
        return out

    def Ay(y):
        assert y.size == n
        out = np.empty(l*m)
        for ll in range(l):
            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])
            out[ll*m:(ll+1)*m] = ay(y)
        return out

    return Ax, Ay, ordering

#SPARC dictionary
#Returns 2 functions Ab and Az which compute Aβ and A^Tz respectively.
def sparc_transforms(L, M, n):
    Ax, Ay, ordering = block_sub_fht(n, M, L, ordering=None)
    def Ab(b):
        return Ax(b).reshape(-1, 1) / np.sqrt(n)
    def Az(z):
        return Ay(z).reshape(-1, 1) / np.sqrt(n)
    return Ab, Az, ordering
# return the ordering
# have second function which does the same but uses the previous ordering. And on fewer sections. 
# Check it on some betas. Use same beta and check it's the same result. Set last sections of beta to zero 
# For the second function just use the first L unprotected rows of the ordering. 


#SPARC dictionary
#Returns 2 functions Ab and Az which compute Aβ and A^Tz respectively.
#Takes in the value of ordering so a smaller design matrix can be generated to run on 
#just the unprotected sections for the second round of amp decoding. 
def sparc_transforms_shorter(L, M, n, ordering):
    # just use the first L unprotected rows of ordering
    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=ordering[:L,:])
    def Ab(b):
        return Ax(b).reshape(-1, 1) / np.sqrt(n)
    def Az(z):
        return Ay(z).reshape(-1, 1) / np.sqrt(n)
    return Ab, Az

# Power allocation
# Just going to include parameterised power allocation (but note there are two other options.)
# This results in an exponential pa with parameterised steepnees which is
# flattened after some point. 
# We generate a power allocation parameterised by a and  f which set 
# the steepness and the flattening point respectively.
# exponential pa up to fL and then flat. The power is scaled to sum to P
def pa_parameterised(L, C, P, a, f):
    pa = 2**(-2 * a * C * np.arange(L) / L)
    pa[int(f*L):] = pa[int(f*L)]
    pa /= pa.sum() / P
    return pa

# the amp algorithm
def amp(y, σ_n, Pl, L, M, T, Ab, Az, β=np.array([None])):
    P = np.sum(Pl)
    n = y.size
    if β.all()==None:
        β = np.zeros((L*M, 1))
        z = y
        last_τ = 0
    else:
        β = β.reshape(L*M,1)
        z = y - Ab(β)
        # just to ensure the if statement isn't true on the first loop
        last_τ = 0
    # delete this line! Just for testing purposes
    #T = 10
    
    for t in range(T):
        τ = np.sqrt(np.sum(z**2)/n)
        #if τ == last_τ:
        # can change params for isclose if need be. If stopping too early.
        if np.isclose(τ, last_τ):
            #print("Stopping after t runs:", t)
            return β
        last_τ = τ

        
        # added astype to increase precision to avoid divide by zero in LLR
        s = (β + Az(z))#.astype(np.longdouble)
        rt_n_Pl = np.sqrt(n*Pl).repeat(M).reshape(-1, 1)
        u = s * rt_n_Pl / τ**2
        max_u = u.max()
        exps = np.exp(u - max_u)
        sums = exps.reshape(L, M).sum(axis=1).repeat(M).reshape(-1, 1)
        β = (rt_n_Pl * exps / sums).reshape(-1, 1)
        z = y - Ab(β) + (z/τ**2) * (P - np.sum(β**2) / n)
    
    return β

#########End of code from Python Tutorial###########

# SPARC parameters
# l is the number of sections in the overall SPARC
# m is the number of columns per section in the SPARC
# p is the channel signal power, sigma2 is the channel noise power
# r is the user data rate, r_pa the power allocation design rate
# t is the maximum number of AMP iterations permitted
class SPARCParams:
	def __init__(self, L, M, sigma, p, r, t, a=None, f=None, C=None):
		self.L = L
		self.M = M
		self.sigma = sigma
		self.p = p
		self.r = r
		self.t = t
		self.a = a
		self.f = f
		self.C = C


# LDPC Params needed to set up one of Jossy's code
class LDPCParams:
    def __init__(self, standard, r_ldpc, z, ptype='A'):
        self.standard = standard
        self.r_ldpc = r_ldpc
        self.z = z
        self.ptype = ptype


# results of an LDPC simulation
class LDPCResult:
	def __init__(self, errors_after_amp_unprotected, errors_after_amp_protected, errors_after_amp_parity,
		errors_after_ldpc_protected, errors_after_ldpc_parity, errors_after_post_amp_unprotected, iters_amp,
		iters_ldpc, iters_post_amp, ldpc_success):
		self.errors_after_amp_unprotected = errors_after_amp_unprotected
		self.errors_after_amp_protected = errors_after_amp_protected
		self.errors_after_amp_parity = errors_after_amp_parity
		self.errors_after_ldpc_protected = errors_after_ldpc_protected
		self.errors_after_ldpc_parity = errors_after_ldpc_parity
		self.errors_after_post_amp_unprotected = errors_after_post_amp_unprotected
		self.iters_amp = iters_amp
		self.iters_ldpc = iters_ldpc
		self.iters_post_amp = iters_post_amp
		self.ldpc_success = ldpc_success



#function to convert sectionwise probabilities to bitwise probabilities
# note: my sp2bp takes the entire beta vector, whereas adams code works only on 1 section. Change this if neccessary
def sp2bp(β, L, M: "must be power of 2"):
    #take in the section posterior probabilities β.
    #and the no. sections L and the size of each section M. 
    #note L may not equal the total number of sections for the sparc code. 
    #normally just interested in the number of sections covered by the LDPC
    
    #initialise numpy array of zeros for all of the bitwise posteriors
    p = np.zeros(int(np.log2(M)*L))
    #loop through the L sections.
    for a in range(L):
        β_l = β[a*M:((a+1)*M)]
        # normalizing before passing in, so below not needed
        #c = np.sum(β_l) #calculate normalization constant for each section
        for logi in range(int(np.log2(M))):
            b = int((a+1)*np.log2(M) - logi - 1)
            i = 2**logi
            k = i
            while k<M:
                #note shift in range of j due to βl being an array which is indexed from 0 not 1. 
                for j in range(k, k+i):
                    p[b] = p[b] + β_l[j]
                k = k + 2*i
    return p


# function to convert bitwise probabilities to sectionwise probabilities
# v is the bitwise probabilities
# L is the number of sections in v
# M is the number of bits in v
def bp2sp(v, L, M: "must be power of 2"):
    logm = int(np.log2(M))
    # sectionwise probabilities
    sp = np.zeros(L*M)

    for l in range(L):
        # bitwise probabilities for section l
        bp = v[l*logm:(l+1)*logm]

        for m in range(M):
            # The bit values that correspond to the mth entry of this section being non-zero
            bits = bitarray(bin(m)[2:].zfill(logm))
            # bp and (1-bp) to the power of bits ensures that only the probability of each bit 
            # being the value we would need it to be for this column to be non-zero is included
            # in the probability. 
            # So we are multiplying the probability of each bit being how we need it to make the
            # current column non-zero.
            #print("bp", bp)
            #print("bits", bits)
            a = bp**bits
            bits.invert()
            b = (1-bp)**(bits)
            sp[l*M+m] = np.prod(a * b)
        sp[l*M:(l+1)*M] = sp[l*M:(l+1)*M]/sum(sp[l*M:(l+1)*M])

    return sp

# assumes that the left most bit (lmb) is the most significant bit (msb)
# note that an input of all zero bits for one section will give an output of 0
# So the first column is being indexed by 0
def bits2indices(bits, m: "m must be a power of 2"):
	
    logm = int(math.log(m,2))
    # assert that the bits can be split exactly into sections of size logm
    assert len(bits)%logm==0
    # find number of sections required for the bits
    sections = int(len(bits)/logm)
    # set up array to store the indices
    indices = []

    for i in range(sections):
        # get the bits corresponding to the current section
        section_bits = bits[i*logm:(i+1)*logm]
        # variable to store the value of the bits in indice
        digit = 0
        for j in range(len(section_bits)):
            if section_bits[j]:
                digit += 2**(len(section_bits)-j-1) 
        indices.append(digit)

    return indices

# note removed r_pa from function as never use it
# a, f, and C control the parameterised power allocation
def amp_ldpc_sim(sparcparams: SPARCParams, ldpcparams: LDPCParams = None, a=None, f=None, C=None):
    #Get the SPARC parameters from the struct
    L = sparcparams.L
    M = sparcparams.M
    P = sparcparams.p
    sigma = sparcparams.sigma
    r_sparc = sparcparams.r
    #r_pa_sparc = sparcparams.r_pa
    T = sparcparams.t
    a = sparcparams.a
    f = sparcparams.f
    C = sparcparams.C

    # calculate some additional parameters 
    # Compute the SNR, capacity, and n, from the input parameters
    snr = P / sigma**2
    #C = 0.5 * np.log2(1 + snr)
    n = int(L*np.log2(M) / r_sparc)
    # compute additional parameters
    logm = np.log2(M)
    total_bits = int(logm*L)
    
    # Generate the power allocation
    # Pl = pa_iterative(L, L, sigma, P, R_PA)
    if a==None: 
        # uniform power allocation across sections
        Pl = P/L * np.ones(L)
    else:
        Pl = pa_parameterised(L, C, P, a, f)


    if ldpcparams == None:
        # set params so that no ldpc encoding or decoding is used
        nl = 0
        kl = 0
        ldpc_bits = []
    else: 
        standard = ldpcparams.standard
        r_ldpc = ldpcparams.r_ldpc
        z = ldpcparams.z
        ptype = ldpcparams.ptype
        # initialise the ldpc code
        ldpc_code = ldpc.code(standard, r_ldpc, z, ptype)
        nl = ldpc_code.N
        kl = ldpc_code.K

        assert nl<=(L*logm)
        # we want the ldpc to cover a complete number of sections and not just part of the final section it covers. 
        assert nl%logm == 0
        #(L, M, sigma, P, R, T, R_PA, R_LDPC):

        # Generate random message in 2 stages
        # First generate ldpc bits
        protected_bits = np.random.randint(0, 2, kl).tolist()
        assert len(protected_bits) == kl

        # Encode the protected_bits using the ldpc code 
        ldpc_bits = ldpc_code.encode(protected_bits).tolist() 

    # Generate the remaining bits required
    unprotected_bits = np.random.randint(0, 2, int(total_bits-nl)).tolist()

    # concatenate the ldpc and unprotected bits      
    sparc_bits = unprotected_bits+ldpc_bits

    assert len(sparc_bits)==total_bits

    # convert the bits to indices for encoding
    sparc_indices = bits2indices(sparc_bits, M)

    assert len(sparc_indices)==L
       
    # Generate the SPARC transform functions A.beta and A'.z
    Ab, Az, ordering = sparc_transforms(L, M, n)
    
    # Generate our transmitted signal X
    β_0 = np.zeros((L*M, 1))
    for l in range(L):
        β_0[l*M + sparc_indices[l]] = np.sqrt(n * Pl[l])
    x = Ab(β_0)
    
    # check that the power has been allocated uniformly. This should be approx equal to one when divided by the snr. 
    #print("Average power/snr is ", np.mean(x**2)/snr)

    # Generate random channel noise and then received signal y
    z = np.random.randn(n, 1) * sigma
    y = (x + z).reshape(-1, 1)
        
    # Run AMP decoding
    β = amp(y, sigma, Pl, L, M, T, Ab, Az).reshape(-1)
    # Output the results of just AMP decoding
    
    # Keep below so I can compare it to using the outer ldpc code. Actually below doesn;t work anymore as I generated bits initially. 
    # Convert decoded beta back to a message
    rx_message = []
    for l in range(L):
        idx = np.argmax(β[l*M:(l+1)*M])
        rx_message.append(idx)
    
    # Compute fraction of sections decoded correctly with just AMP decoding
    #correct_amp = np.sum(np.array(rx_message) == np.array(sparc_indices)) / L
    #print("Fraction of sections decoded correctly with amp: ", correct_amp)
    
    # Compute BER for just sparc
    ber_amp = sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, rx_message))/total_bits


    if ldpcparams==None:
        ber_ldpc = None
        ber_ldpc_amp = None
    else:
        # Get the sectionwise posterior probabilities by dividing β by the power in each section. 
        # This converts each section in β to a valid probability distribution.
        sectionwise_posterior = β/np.sqrt(n*np.repeat(Pl, M))

        ldpc_sections = int(nl/logm)
        # convert sectionwise to bitwise posterior probabilities
        # only need the sections corresponding to ldpc code 
        bitwise_posterior = sp2bp(sectionwise_posterior[(L-ldpc_sections)*M:], ldpc_sections, M) 

        np.clip(bitwise_posterior, 0.001, 1-0.001, out=bitwise_posterior)
        # computer the log likelihood ratio for decoding
        LLR = np.log(1-bitwise_posterior)- np.log(bitwise_posterior)

        (app, it) = ldpc_code.decode(LLR)

        #test_output = (bitwise_posterior>0.5)
        #print("Incorrect bits before ldpc decoding: ", np.sum(test_output!=ldpc_bits))

        # check that the inequality is the right way round in below
        v_output = (app<0.0)
        #print("Incorrect bits in ldpc decoding: ", np.sum(v_output!=ldpc_bits))

        # convert the bits to indices
        beta_output_ldpc = bits2indices(v_output, M)

        #### Compute performance with just amp and then ldpc
        beta_output = rx_message
        beta_output[L-ldpc_sections:] = beta_output_ldpc
        # compute fraction of sections decoded correctly with AMP followed by SPARC decoding
        #correct_ldpc = np.sum(np.array(beta_output) == np.array(sparc_indices))/L
        #print("Fractions of sections decoded correctly with amp and ldpc: ", correct_ldpc)

        # compute BER for amp and ldpc
        ber_ldpc = sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, beta_output))/total_bits

        # only perform final round of AMP decoding if not all sections of beta are covered with LDPC
        if (L-ldpc_sections==0):
            ber_ldpc_amp = None
        else:    
            # generate beta_ldpc which has the first L unprotected sections set to zero
            # and the L ldpc sections will have exactly one non-zero entry per section. 
            beta_ldpc = np.zeros((L*M, 1))
            # using variable i because beta_output_L only has ldpc_sections number of entries. 
            i=0
            for l in range(L-ldpc_sections,L):
                beta_ldpc[(l)*M + beta_output_ldpc[i]] = np.sqrt(n * Pl[l])
                i+=1
            x_ldpc = Ab(beta_ldpc)

            # calculate a new value of the channel output which doesn't contain the contribution from 
            # the ldpc sections.
            y_new = y - x_ldpc

            # Rerun amp decoding on the new input y_new over the unprotected sections.
            L_unprotected = L-ldpc_sections
            Ab_new, Az_new = sparc_transforms_shorter(L_unprotected, M, n, ordering)

            beta_new = amp(y_new, sigma, Pl[:L_unprotected], L_unprotected, M, T, Ab_new, Az_new).reshape(-1)

            # Convert decoded beta_new back to a message
            # beta_new gives you the first L-ldpc_sections sections of the final received message. 
            rx_message_final = []
            for l in range(L_unprotected):
                idx = np.argmax(beta_new[l*M:(l+1)*M])
                rx_message_final.append(idx)


            #beta_output = np.zeros(L)
            beta_output[:L_unprotected] = rx_message_final
            # don't need to do line below as they're already equal
            #beta_output[L-ldpc_sections:] = beta_output_ldpc
            # compute fraction of sections decoded correctly with AMP followed by SPARC decoding followed by amp.
            #correct_ldpc = np.sum(np.array(beta_output) == np.array(sparc_indices))/L
            #print("Fractions of sections decoded correctly with amp and ldpc and amp: ", correct_ldpc)

                
            # compute BER for amp and ldpc and amp again.
            ber_ldpc_amp = sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, beta_output))/total_bits      

    # overall rate of the code
    R = (L*logm - (nl-kl))/n

    #Compute Eb/N0
    #EbN0 = 1/(2*R) * (P/sigma**2)
   
    return ber_amp, ber_ldpc, ber_ldpc_amp, R

# simulation of sparc code with outer amp code with soft information exchange 
# between the amp decoder and the ldpc decoder
# soft_iter The number of iterations of soft information exchange
# in this function '_ldpc' means after ldpc decoding and '_amp' means after amp decoding
# a, f, and C control the parameterised power allocation
def soft_amp_ldpc_sim(sparcparams: SPARCParams, ldpcparams: LDPCParams, soft_iter, a=None, f=None, C=None):
    # arrays to store the return values of the ber
    # bit error rate after each round of amp decoding
    ber_amp = []
    # ber after each round of ldpc decoding
    ber_ldpc = []

    #Get the SPARC parameters from the struct
    L = sparcparams.L
    M = sparcparams.M
    P = sparcparams.p
    sigma = sparcparams.sigma
    r_sparc = sparcparams.r
    #r_pa_sparc = sparcparams.r_pa
    T = sparcparams.t
    a = sparcparams.a
    f = sparcparams.f
    C = sparcparams.C

    # calculate some additional parameters 
    # Compute the SNR, capacity, and n, from the input parameters
    snr = P / sigma**2
    #C = 0.5 * np.log2(1 + snr)
    n = int(L*np.log2(M) / r_sparc)
    # compute additional parameters
    logm = np.log2(M)
    total_bits = int(logm*L)
    
    # Generate the power allocation
    # Pl = pa_iterative(L, L, sigma, P, R_PA)
    if a==None:
        # uniform power allocation across sections
        Pl = P/L * np.ones(L)
    else:
        # parameterised power allocation
        Pl = pa_parameterised(L, C, P, a, f)

    standard = ldpcparams.standard
    r_ldpc = ldpcparams.r_ldpc
    z = ldpcparams.z
    ptype = ldpcparams.ptype
    # initialise the ldpc code
    ldpc_code = ldpc.code(standard, r_ldpc, z, ptype)
    nl = ldpc_code.N
    kl = ldpc_code.K

    assert nl<=(L*logm)
    # we want the ldpc to cover a complete number of sections and not just part of the final section it covers. 
    assert nl%logm == 0
    #(L, M, sigma, P, R, T, R_PA, R_LDPC):

    # Generate random message in 2 stages
    # First generate ldpc bits
    protected_bits = np.random.randint(0, 2, kl).tolist()
    assert len(protected_bits) == kl

    # Encode the protected_bits using the ldpc code 
    ldpc_bits = ldpc_code.encode(protected_bits).tolist() 

    # Generate the remaining bits required
    unprotected_bits = np.random.randint(0, 2, int(total_bits-nl)).tolist()

    # concatenate the ldpc and unprotected bits      
    sparc_bits = unprotected_bits+ldpc_bits

    assert len(sparc_bits)==total_bits

    # convert the bits to indices for encoding
    sparc_indices = bits2indices(sparc_bits, M)

    assert len(sparc_indices)==L
       
    # Generate the SPARC transform functions A.beta and A'.z
    Ab, Az, ordering = sparc_transforms(L, M, n)
    
    # Generate our transmitted signal X
    β_0 = np.zeros((L*M, 1))
    for l in range(L):
        β_0[l*M + sparc_indices[l]] = np.sqrt(n * Pl[l])
    x = Ab(β_0)
    
    # check that the power has been allocated uniformly. This should be approx equal to one when divided by the snr. 
    #print("Average power/snr is ", np.mean(x**2)/snr)

    # Generate random channel noise and then received signal y
    z = np.random.randn(n, 1) * sigma
    y = (x + z).reshape(-1, 1)
    
    # Run AMP decoding
    β = amp(y, sigma, Pl, L, M, T, Ab, Az).reshape(-1)
    # Output the results of just AMP decoding
    
    # Convert decoded beta back to a message
    rx_message = []
    for l in range(L):
        idx = np.argmax(β[l*M:(l+1)*M])
        rx_message.append(idx)
    
    # Compute fraction of sections decoded correctly with just AMP decoding
    #correct_amp = np.sum(np.array(rx_message) == np.array(sparc_indices)) / L
    #print("Fraction of sections decoded correctly with amp: ", correct_amp)
    
    # Compute BER for just sparc
    ber_amp.append(sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, rx_message))/total_bits)

    beta_amp = β
    rx_message_amp = rx_message
    for i in range(soft_iter):
        # Get the sectionwise posterior probabilities by dividing β by the power in each section. 
        # This converts each section in β to a valid probability distribution.
        sectionwise_posterior = beta_amp/np.sqrt(n*np.repeat(Pl, M))

        #print('sectionwise posterior sum ', sum(sectionwise_posterior))

        # remember only want to perform ldpc decoding on sections covered by the ldpc code
        ldpc_sections = int(nl/logm)
        # convert sectionwise to bitwise posterior probabilities
        # only need the sections corresponding to ldpc code 
        bitwise_posterior = sp2bp(sectionwise_posterior[(L-ldpc_sections)*M:], ldpc_sections, M) 

        np.clip(bitwise_posterior, 0.001, 1-0.001, out=bitwise_posterior)
        # computer the log likelihood ratio for decoding
        LLR = np.log(1-bitwise_posterior)- np.log(bitwise_posterior)

        (app, it) = ldpc_code.decode(LLR)

        # get hard decision on bits after ldpc
        v_output = (app<0.0)
        
        # convert the bits to indices
        v_indices = bits2indices(v_output, M)

        # replace old indices for ldpc sections with new ones
        ldpc_indices = rx_message_amp
        ldpc_indices[L-ldpc_sections:] = v_indices

        # compute BER after ldpc decoding
        ber_ldpc.append(sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, ldpc_indices))/total_bits)

        # now perform amp decoding using the soft output from the ldpc
        # get the bitwise a posterior probabilities from the a posterior log likelihood ratios in app
        bitwise_ldpc = 1/(1+np.exp(app))

        # check bitwise_ldpc is made up of valid probabilities
        assert(bitwise_ldpc.all()>=0 and bitwise_ldpc.all()<=1)

        # convert the bitwise posterior to sectionwise posterior
        sectionwise_ldpc = sectionwise_posterior
        sectionwise_ldpc[M*(L-ldpc_sections):] = bp2sp(bitwise_ldpc, ldpc_sections, M)
        #print("sum sectionwise_ldpc ", sum(sectionwise_ldpc))

        # multiply by the power allocation
        beta_ldpc = sectionwise_ldpc * np.sqrt(n*np.repeat(Pl[(L-ldpc_sections):], M))

        # pass this through the sparc transform to give a new channel input for this approximation of beta
        # Note Ab is just created based on size so don't need to regenerate for this new input
        #x_ldpc = Ab(beta_ldpc)
        # This time we don't want to add any noise as we aren't trying to simulate the channel, we are just trying to perform another round of decoding
        # Run AMP decoding
        beta_amp = amp(y, sigma, Pl, L, M, T, Ab, Az, beta_ldpc).reshape(-1)

        # Convert decoded beta_amp back to a message 
        rx_message_amp = []
        for l in range(L):
            idx = np.argmax(beta_amp[l*M:(l+1)*M])
            rx_message_amp.append(idx)

        ber_amp.append(sum(bin(a^b).count('1') for (a, b) in zip(sparc_indices, rx_message_amp))/total_bits) 

    # overall rate of the code
    R = (L*logm - (nl-kl))/n

    #Compute Eb/N0
    #EbN0 = 1/(2*R) * (P/sigma**2)
   
    return ber_amp, ber_ldpc, R


 # Code for simulating the AWGN channel   
def awgn(x, sigma):
    noise = sigma*np.random.randn(len(x))
    return np.add(x, noise)

def ch2llr(ch, sigma):
    sigma2 = sigma**2
    return 2.0/sigma2*ch

def bpsk(x):
    return 1.0 - 2.0*x  

# the simulation of transmitting over the AWGN channel
# Change this so it only returns the BER for 1 simulation. 
# Then I can do the looping else where and plot my own graph
def sim_ldpc(ldpcparams: LDPCParams, sigma, MIN_ERRORS = 100, MAX_BLOCKS = 400000):
    
    rate = ldpcparams.r_ldpc
    standard = ldpcparams.standard
    z = ldpcparams.z
    ptype = ldpcparams.ptype

    if rate == "1/2":
        R = .5
    elif rate == "2/3":
        R = 0.6667
    elif rate == "3/4":
        R = 0.75
    elif rate == "5/6":
        R = 0.83333
    else:
        raise NameError("Rate unsupported")

    mycode = ldpc.code(standard, rate, z, ptype)
    K = mycode.K
    N = mycode.N


    nbiterrors = 0
    nblockerrors = 0
    nblocks = 0
    nit_total = 0
    while nblockerrors < MIN_ERRORS:
        u = np.random.randint(0,2,K)
        x = mycode.encode(u)
        xm = bpsk(x)

        y = awgn(xm, sigma)

        yl = ch2llr(y, sigma)
        (app,nit) = mycode.decode(yl, 'sumprod2')
        xh = (app < 0.0)

        biterrors_thisblock = np.sum(x != xh)
        nbiterrors += biterrors_thisblock
        if biterrors_thisblock:
            nblockerrors += 1
        nblocks += 1
        nit_total += nit

        # errors per block divided by the total number of information bits.
        ber = nbiterrors/(nblocks*N)

        if nblocks >= MAX_BLOCKS:
            break

    return ber

# code to plot a waterfall curve for an ldpc code with a bpsk modulation scheme and sparc with an outer ldpc code
def waterfall(sparcparams: SPARCParams, ldpcparams: LDPCParams, csv_filename: str, png_filename: str, datapoints=10, MIN_ERRORS=100, MAX_BLOCKS=500):

    # Sparc parameters
    L = sparcparams.L
    M = sparcparams.M
    logm = np.log2(M)
    p=sparcparams.p
    r_sparc = sparcparams.r
    T = sparcparams.t
    a = sparcparams.a
    f = sparcparams.f
    C = sparcparams.C

    # LDPC parameters
    standard = ldpcparams.standard
    r_ldpc = ldpcparams.r_ldpc
    # number of ldpc sections, must be divisible by 8 to ensure nl is divisible by 24.
    # covering all sections to give overall rate of 5/6
    sec = L
    # number of ldpc bits, must be divisible by 24
    nl = logm * sec
    z = int(nl/24)
    ldpcparams = LDPCParams(standard, r_ldpc, z)    

    n = L*logm/r_sparc
    R = (L*logm-nl*(1-5/6))/n
    # note that R should be 5/6
    print('Overall rate is: ', R)

    # snr at capacity
    snrc = (2**(2*R) - 1)
    EbN0c = 1/(2*R) * snrc
    EbN0c_dB = 20*log10(EbN0c)

    SIGMA = linspace(0.8, 0.4, datapoints)
    # BER after second round of AMP decoding
    BER_amp_2 = np.zeros(datapoints)
    # BER after first round of ldpc
    BER_ldpc = np.zeros(datapoints)
    # BER after 2nd round ldpc
    BER_ldpc_2 = np.zeros(datapoints)
    BER_bpsk = np.zeros(datapoints)
    # BER of plain SPARC
    BER_plain = np.zeros(datapoints)

    i=0
    for sigma in SIGMA:
        # get the BER for the ldpc and bpsk modulation. The repeats are built into the code
        BER_bpsk[i] = sim_ldpc(ldpcparams, sigma, MIN_ERRORS, MAX_BLOCKS)

        sparcparams = SPARCParams(L, M, sigma, p, r_sparc, T, a, f, C)
        # plain sparc with same overall rate for comparison
        sparcparams_plain = SPARCParams(L, M, sigma, p, R, T, a, f, C)
        # cumulative BER for amp and ldpc
        ber_cum_amp = 0
        ber_cum_ldpc = 0
        ber_cum_ldpc2 = 0
        ber_cum_plain = 0
        nblockerrors_ldpc = 0
        nblocks = 0
        while nblockerrors_ldpc < MIN_ERRORS:
            (ber_thisblock_amp, ber_thisblock_ldpc, _) = soft_amp_ldpc_sim(sparcparams, ldpcparams, 2)
            (ber_thisblock_plain, _, _, _) = amp_ldpc_sim(sparcparams_plain)
            #print("BER amp:", ber_thisblock_amp)
            #print("BER ldpc: ", ber_thisblock_ldpc)
            ber_thisblock_amp = ber_thisblock_amp[1]

            ber_cum_amp += ber_thisblock_amp
            ber_cum_ldpc += ber_thisblock_ldpc[0]
            ber_cum_ldpc2 += ber_thisblock_ldpc[1]
            ber_cum_plain += ber_thisblock_plain
            if ber_thisblock_ldpc[0]:
                nblockerrors_ldpc += 1
            nblocks += 1

            if nblocks >= MAX_BLOCKS:
                break

        BER_amp_2[i] = ber_cum_amp/nblocks
        BER_ldpc[i] = ber_cum_ldpc/nblocks
        BER_ldpc_2[i] = ber_cum_ldpc2/nblocks
        BER_plain[i] = ber_cum_plain/nblocks


        i+=1

    #snrdB = 20*np.log10(P/sigma**2)
    EbN0 = 1/(2*R) * (p/SIGMA**2)
    #print(EbN0)
    EbN0_dB = 20*log10(EbN0)
    # open file you want to write CSV output to. 'a' means its in append mode. Switching this to 'w' will make it overwrite the file.
    myFile = open(csv_filename, 'a')
    with myFile:
        myFields = ['EbN0_dB', 'BER_ldpc', 'BER_amp_2', 'BER_ldpc_2', 'BER_plain', 'BER_bpsk']
        writer = csv.DictWriter(myFile, fieldnames=myFields)
        writer.writeheader()
        for k in range(datapoints):
            writer.writerow({'EbN0_dB' : EbN0_dB[k], 'BER_ldpc' : BER_ldpc[k], 'BER_amp_2': BER_amp_2[k], 'BER_ldpc_2': BER_ldpc_2[k], 'BER_plain': BER_plain[k], 'BER_bpsk': BER_bpsk[k]})


    fig, ax = plt.subplots()
    ax.set_yscale('log', basey=10)
    ax.plot(EbN0_dB, BER_ldpc, 'k:', label = 'SPARC w/ outer code: after LDPC')
    ax.plot(EbN0_dB, BER_amp_2, 'k--', label = 'SPARC w/ outer code: after 2nd round of AMP')
    ax.plot(EbN0_dB, BER_ldpc_2, 'k-', label = 'SPARC w/ outer code: after 2nd round of LDPC')
    ax.plot(EbN0_dB, BER_plain, 'b-', label = 'Plain SPARC')
    ax.plot(EbN0_dB, BER_bpsk, 'g-', label = 'LDPC with BPSK modulation')
    plt.axvline(x=EbN0c_dB, color='r', linestyle='-', label='Shannon limit')
    plt.xlabel('$E_b/N_0$ (dB)', fontsize=15) # at some point need to work out how to write this so it outputs properly
    plt.ylabel('BER', fontsize=15)
    plt.tight_layout()
    plt.legend(loc=1, prop={'size': 8})
    plt.savefig(png_filename)


# sec is the number of LDPC sections. Number of ldpc sections, must be divisible by 8 to ensure nl is divisible by 24.
# soft and hard are booleans determining if the function should calculate hard and soft BER
# Note that you need to change the plotting depending on the number of rounds that you want to plot 
def soft_hard_plot(soft: bool, hard: bool, sec: int, soft_iter: int, sparcparams: SPARCParams, ldpcparams: LDPCParams, csv_filename: str, png_filename: str, datapoints=10, MIN_ERRORS=100, MAX_BLOCKS=500):
    # Sparc parameters
    L = sparcparams.L
    M = sparcparams.M
    logm = np.log2(M)
    p=sparcparams.p
    r_sparc = sparcparams.r
    T = sparcparams.t
    a = sparcparams.a
    f = sparcparams.f
    C = sparcparams.C 

    # LDPC parameters
    standard = ldpcparams.standard
    r_ldpc = ldpcparams.r_ldpc
    # number of ldpc bits, must be divisible by 24
    nl = logm * sec
    z = int(nl/24)
    ldpcparams = LDPCParams(standard, r_ldpc, z)

    n = L*logm/r_sparc
    R = (L*logm-nl*(1-5/6))/n

    print('Overall rate is: ', R)

    # snr at capacity
    snrc = (2**(2*R) - 1)
    EbN0c = 1/(2*R) * snrc
    EbN0c_dB = 20*log10(EbN0c)

    SIGMA = linspace(0.8, 0.4, datapoints)
    BER_sparc = np.zeros(datapoints)
    if soft:
        # Array of BER after ldpc decoding
        BER_ldpc_soft = np.zeros((datapoints, soft_iter))
        # Array of BER after AMP decoding
        BER_amp_soft = np.zeros((datapoints, soft_iter+1))
        
    if hard:
        BER_amp_hard = np.zeros((datapoints, 2))
        BER_ldpc_hard = np.zeros(datapoints)

    i=0
    for sigma in SIGMA:
        sparcparams = SPARCParams(L, M, sigma, p, r_sparc, T, a, f, C)
        # sparcparams for running at same overall rate but with no outer code
        sparcparams1 = SPARCParams(L, M, sigma, p, R, T, a, f, C)

        ber_sparc = np.zeros(MIN_ERRORS)
        for j in range(MIN_ERRORS):
            (ber_sparc[j],_,_,_) = amp_ldpc_sim(sparcparams1)
        BER_sparc[i] = np.sum(ber_sparc)/MIN_ERRORS

        if soft:
            # cumulative BER for amp and ldpc
            ber_cum_amp = np.zeros((soft_iter+1))
            ber_cum_ldpc = np.zeros(soft_iter)
            nblockerrors_ldpc = 0
            nblocks = 0
            while nblockerrors_ldpc < MIN_ERRORS:
                (ber_thisblock_amp, ber_thisblock_ldpc, _) = soft_amp_ldpc_sim(sparcparams, ldpcparams, soft_iter)
                #print("BER amp:", ber_thisblock_amp)
                #print("BER ldpc: ", ber_thisblock_ldpc

                ber_cum_amp += ber_thisblock_amp
                ber_cum_ldpc += ber_thisblock_ldpc
                if ber_thisblock_ldpc[0]:
                    nblockerrors_ldpc += 1
                nblocks += 1

                if nblocks >= MAX_BLOCKS:
                    break

            BER_amp_soft[i,:] = ber_cum_amp/nblocks
            BER_ldpc_soft[i,:] = ber_cum_ldpc/nblocks
        if hard:
            # cumulative BER for amp and ldpc
            ber_cum_amp = np.zeros(2)
            ber_cum_ldpc = 0
            ber_thisblock_amp = np.zeros(2)
            ber_thisblock_ldpc = 0
            nblockerrors_ldpc = 0
            nblocks = 0
            while nblockerrors_ldpc < MIN_ERRORS:
                (ber_thisblock_amp[0], ber_thisblock_ldpc, ber_thisblock_amp[1], _) = amp_ldpc_sim(sparcparams, ldpcparams)

                ber_cum_amp += ber_thisblock_amp
                ber_cum_ldpc += ber_thisblock_ldpc
                if ber_thisblock_ldpc:
                    nblockerrors_ldpc += 1
                nblocks += 1

                if nblocks >= MAX_BLOCKS:
                    break
            BER_amp_hard[i,:] = ber_cum_amp/nblocks
            BER_ldpc_hard[i] = ber_cum_ldpc/nblocks

        i+=1

    #snrdB = 20*np.log10(P/sigma**2)
    EbN0 = 1/(2*R) * (p/SIGMA**2)
    #print(EbN0)
    EbN0_dB = 20*log10(EbN0)
    # open file you want to write CSV output to. 'a' means its in append mode. Switching this to 'w' will make it overwrite the file.
    myFile = open(csv_filename, 'a')
    with myFile:
        if soft: 
            myFields = ['EbN0_dB', 'BER_sparc', 'BER_ldpc_soft', 'BER_amp_soft']
            writer = csv.DictWriter(myFile, fieldnames=myFields)
            writer.writeheader()
            for k in range(datapoints):
                writer.writerow({'EbN0_dB' : EbN0_dB[k], 'BER_sparc' : BER_sparc[k],'BER_ldpc_soft' : BER_ldpc_soft[k,:], 'BER_amp_soft': BER_amp_soft[k,:]})
        if hard: 
            myFields = ['EbN0_dB', 'BER_sparc', 'BER_ldpc_hard', 'BER_amp_hard']
            writer = csv.DictWriter(myFile, fieldnames=myFields)
            writer.writeheader()
            for k in range(datapoints):
                writer.writerow({'EbN0_dB' : EbN0_dB[k], 'BER_sparc' : BER_sparc[k],'BER_ldpc_hard' : BER_ldpc_hard[k], 'BER_amp_hard': BER_amp_hard[k,:]})


    fig, ax = plt.subplots()
    ax.set_yscale('log', basey=10)
    ax.plot(EbN0_dB, BER_sparc, 'g:', label = 'SPARC w/ same overall rate')
    if soft:
        # comment out and add lines below as appropriate
        ax.plot(EbN0_dB, BER_amp_soft[:,0], 'b:', label = 'SPARC w/ outer code: after AMP')
        ax.plot(EbN0_dB, BER_ldpc_soft[:,0], 'k:', label = 'SPARC w/ outer code: after LDPC (soft)')
        ax.plot(EbN0_dB, BER_amp_soft[:,1], 'b--', label = 'SPARC w/ outer code: after 2nd round of AMP (soft)')
        ax.plot(EbN0_dB, BER_ldpc_soft[:,1], 'k--', label = 'SPARC w/ outer code: after 2nd round of LDPC (soft)')
        ax.plot(EbN0_dB, BER_amp_soft[:,2], 'b-', label = 'SPARC w/ outer code: after 3rd round of AMP (soft)')
    if hard:
        #ax.plot(EbN0_dB, BER_amp_hard[:,0], 'c:', label = 'SPARC w/ outer code: after AMP (hard)')
        ax.plot(EbN0_dB, BER_ldpc_hard, 'm:', label = 'SPARC w/ outer code: after LDPC (hard)')
        ax.plot(EbN0_dB, BER_amp_hard[:,1], 'c--', label = 'SPARC w/ outer code: after 2nd round of AMP (hard)')
    plt.axvline(x=EbN0c_dB, color='r', linestyle='-', label='Shannon limit')
    plt.xlabel('$E_b/N_0$ (dB)') # at some point need to work out how to write this so it outputs properly
    plt.ylabel('BER')
    plt.tight_layout()
    plt.legend(loc=1, prop={'size': 7})
    plt.savefig(png_filename)


if __name__ == "__main__":
    # get the time so you can calculate the wall clock time of the process
    t0 = time.time()

    '''
    #######################################
    # Plot plain SPARCs with different overall rates for a high number of repeats

    repeats = 200
    datapoints=15
    SIGMA = linspace(0.8, 0.4, datapoints)
    i=0
    ber_sparc1 = np.zeros(datapoints)
    ber_sparc2 = np.zeros(datapoints)
    ber_sparc3 = np.zeros(datapoints)
    for sigma in SIGMA:
        sparcparams1 = SPARCParams(L=768, M=512, sigma=sigma, p=1.8, r=5/6, t=64)
        sparcparams2 = SPARCParams(L=768, M=512, sigma=sigma, p=1.8, r=0.877, t=64)
        sparcparams3 = SPARCParams(L=768, M=512, sigma=sigma, p=1.8, r=0.75, t=64)

        ber_sparc1_cum = 0
        ber_sparc2_cum = 0
        ber_sparc3_cum = 0
        


        for j in range(repeats):
            (ber_thisblock1, _, _, _) = amp_ldpc_sim(sparcparams1)
            (ber_thisblock2, _, _, _) = amp_ldpc_sim(sparcparams1)
            (ber_thisblock3, _, _, _) = amp_ldpc_sim(sparcparams1)

            ber_sparc1_cum += ber_thisblock1
            ber_sparc2_cum += ber_thisblock2
            ber_sparc3_cum += ber_thisblock3

        ber_sparc1[i] = ber_sparc1_cum/repeats
        ber_sparc2[i] = ber_sparc2_cum/repeats
        ber_sparc3[i] = ber_sparc3_cum/repeats
        i+=1

    p=1.8
    snr = (p/SIGMA**2)
    snr_dB = 20*log10(snr)


    fig, ax = plt.subplots()
    ax.set_yscale('log', basey=10)
    ax.plot(snr_dB, ber_sparc1, 'b-', label = 'SPARC rate 5/6')
    ax.plot(snr_dB, ber_sparc2, 'g-', label = 'SPARC rate 0.877')
    ax.plot(snr_dB, ber_sparc3, 'k-', label = 'SPARC rate 0.75')
    plt.xlabel('SNR (dB)', fontsize=15) # at some point need to work out how to write this so it outputs properly
    plt.ylabel('BER', fontsize=15)
    plt.tight_layout()
    plt.legend(loc=1, prop={'size': 8})
    plt.savefig('Plain_sparc_diff_rates.png')
    
    
    '''
    '''
    ##########################################
    # Plot waterfall curves
    # Compare ldpc with bpsk of rate 5/6 to a sparc with sparc rate 1 and ldpc rate 5/6 with all sections covered
    # to give a overall rate of 5/6
    # And SPARC with no outer code and overall rate 5/6
    # Note that z is set within the waterfall function so just set as None here
    ldpcparams = LDPCParams('802.16', '5/6', None)
    sparcparams = SPARCParams(L=768, M=512, sigma=None, p=1.8, r=1, t=64)
    waterfall(sparcparams, ldpcparams, datapoints=15, MIN_ERRORS=200, MAX_BLOCKS=200, csv_filename='EbN0_dBVsBER_waterfall_rep200_5.csv', png_filename='EbN0_dBVsBER_waterfall_rep200_5.png')

    print("Wall clock time elapsed: ", time.time()-t0)
    '''
    
    #########################################
    # Plot hard and soft loops
    ldpcparams = LDPCParams('802.16', '5/6', None)
    sparcparams = SPARCParams(L=768, M=512, sigma=None, p=1.8, r=1, t=64)
    soft_hard_plot(soft=True, hard=True, sec=569, soft_iter=2, sparcparams= sparcparams, ldpcparams=ldpcparams, csv_filename='EbN0VsBER_soft_hard_100_4.csv', png_filename='EbN0VsBER_soft_hard_100_4.png', datapoints=10, MIN_ERRORS=100, MAX_BLOCKS=100)
    
    print("Wall clock time elapsed: ", time.time()-t0)
    
    
import numpy as n
import numpy as np
from math import log


#Code taken from Adam's python tutorial

try:
    from pyfht import fht_inplace
except ImportError:
    import warnings
    warnings.warn("Using very slow Python version of fht, please install pyfht")
    def fht_inplace(x):
        N = len(x)
        i = N>>1
        while i:
            for k in range(0, N, 2*i):
                for j in range(k, i+k):
                    ij = i|j
                    temp = x[j]
                    x[j] += x[ij]
                    x[ij] = temp - x[ij]
            i = i >> 1


def sub_fht(n, m, seed=0, ordering=None):
    """
    Returns functions to compute the sub-sampled Walsh-Hadamard transform,
    i.e., operating with a wide rectangular matrix of random +/-1 entries.

    n: number of rows
    m: number of columns

    It is most efficient (but not required) for max(m+1,n+1) to be a power of 2.

    seed: determines choice of random matrix
    ordering: optional n-long array of row indices in [1, max(m,n)] to
              implement subsampling; generated by seed if not specified,
              but may be given to speed up subsequent runs on the same matrix.

    Returns (Ax, Ay, ordering):
        Ax(x): computes A.x (of length n), with x having length m
        Ay(y): computes A'.y (of length m), with y having length n
        ordering: the ordering in use, which may have been generated from seed
    """
    assert n > 0, "n must be positive"
    assert m > 0, "m must be positive"
    w = 2**int(np.ceil(np.log2(max(m+1, n+1))))

    if ordering is not None:
        assert ordering.shape == (n,)
    else:
        rng = np.random.RandomState(seed)
        idxs = np.arange(1, w, dtype=np.uint32)
        rng.shuffle(idxs)
        ordering = idxs[:n]

    def Ax(x):
        assert x.size == m, "x must be m long"
        y = np.zeros(w)
        y[w-m:] = x.reshape(m)
        fht_inplace(y)
        return y[ordering]

    def Ay(y):
        assert y.size == n, "input must be n long"
        x = np.zeros(w)
        x.flat[ordering] = y
        fht_inplace(x)
        return x[w-m:]

    return Ax, Ay, ordering

def block_sub_fht(n, m, l, seed=0, ordering=None):
    """
    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially
    offering substantial speed improvements.

    n: number of rows
    m: number of columns per block
    l: number of blocks

    It is most efficient (though not required) when max(m+1,n+1) is a power of 2.

    seed: determines choice of random matrix
    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to
              implement subsampling; generated by seed if not specified, but
              may be given to speed up subsequent runs on the same matrix.

    Returns (Ax, Ay, ordering):
        Ax(x): computes A.x (of length n), with x having length l*m
        Ay(y): computes A'.y (of length l*m), with y having length n
        ordering: the ordering in use, which may have been generated from seed
    """
    assert n > 0, "n must be positive"
    assert m > 0, "m must be positive"
    assert l > 0, "l must be positive"

    if ordering is not None:
        assert ordering.shape == (l, n)
    else:
        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))
        rng = np.random.RandomState(seed)
        ordering = np.empty((l, n), dtype=np.uint32)
        idxs = np.arange(1, w, dtype=np.uint32)
        for ll in range(l):
            rng.shuffle(idxs)
            ordering[ll] = idxs[:n]
        

    def Ax(x):
        assert x.size == l*m
        out = np.zeros(n)
        for ll in range(l):
            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])
            out += ax(x[ll*m:(ll+1)*m])
        return out

    def Ay(y):
        assert y.size == n
        out = np.empty(l*m)
        for ll in range(l):
            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])
            out[ll*m:(ll+1)*m] = ay(y)
        return out

    return Ax, Ay, ordering

#SPARC dictionary
#Returns 2 functions Ab and Az which compute Aβ and A^Tz respectively.
def sparc_transforms(L, M, n):
    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)
    def Ab(b):
        return Ax(b).reshape(-1, 1) / np.sqrt(n)
    def Az(z):
        return Ay(z).reshape(-1, 1) / np.sqrt(n)
    return Ab, Az

#Power allocation, we have 3 options

#Original
def pa_original(L, C, P):
    #pa = 2**(-2 * a * C * np.arange(L) / L)
    #above was adam's code but I have removed the a because I think this was wrong 
    pa = 2**(-2 * C * np.arange(L) / L)
    pa /= pa.sum() / P
    return pa

#Parameterised 
def pa_parameterised(L, C, P, a, f):
    pa = 2**(-2 * a * C * np.arange(L) / L)
    pa[int(f*L):] = pa[int(f*L)]
    pa /= pa.sum() / P
    return pa

#iterative
def pa_iterative(L, B, σ, P, R_PA):
    PA = np.zeros(L)
    τ = np.zeros(B)
    k = L//B
    for b in range(B):
        Premain = P - PA.sum()
        τ[b] = np.sqrt(σ**2 + Premain)
        Pblock = 2 * np.log(2) * (R_PA/L) * τ[b]**2
        Pspread = Premain / (L - k*b)
        if Pblock > Pspread:
            PA[k*b:k*(b+1)] = Pblock
        else:
            PA[k*b:] = Pspread
            break
    return PA


# the amp algorithm
def amp(y, σ_n, Pl, L, M, T, Ab, Az):
    P = np.sum(Pl)
    n = y.size
    β = np.zeros((L*M, 1))
    z = y
    last_τ = 0
    
    for t in range(T):
        τ = np.sqrt(np.sum(z**2)/n)
        if τ == last_τ:
            return β
        last_τ = τ
        
        s = β + Az(z)
        rt_n_Pl = np.sqrt(n*Pl).repeat(M).reshape(-1, 1)
        u = s * rt_n_Pl / τ**2
        max_u = u.max()
        exps = np.exp(u - max_u)
        sums = exps.reshape(L, M).sum(axis=1).repeat(M).reshape(-1, 1)
        β = (rt_n_Pl * exps / sums).reshape(-1, 1)
        z = y - Ab(β) + (z/τ**2) * (P - np.sum(β**2) / n)
    
    return β

#########End of code from Python Tutorial###########

# LDPC parameters
# l is the number of sections in the overall SPARC
# m is the number of columns per section in the SPARC
# p is the channel signal power, sigma2 is the channel noise power
# r is the user data rate, r_pa the power allocation design rate
# t is the maximum number of AMP iterations permitted
class LDPCParams:
	def __init__(self, l, m, sigma2, p, r, r_pa, t):
		self.l = l
		self.m = m
		self.sigma2 = sigma2
		self.p = p
		self.r = r
		self.r_pa = r_pa
		self.t


# results of an LDPC simulation
class LDPCResult:
	def __init__(self, errors_after_amp_unprotected, errors_after_amp_protected, errors_after_amp_parity,
		errors_after_ldpc_protected, errors_after_ldpc_parity, errors_after_post_amp_unprotected, iters_amp,
		iters_ldpc, iters_post_amp, ldpc_success):
		self.errors_after_amp_unprotected = errors_after_amp_unprotected
		self.errors_after_amp_protected = errors_after_amp_protected
		self.errors_after_amp_parity = errors_after_amp_parity
		self.errors_after_ldpc_protected = errors_after_ldpc_protected
		self.errors_after_ldpc_parity = errors_after_ldpc_parity
		self.errors_after_post_amp_unprotected = errors_after_post_amp_unprotected
		self.iters_amp = iters_amp
		self.iters_ldpc = iters_ldpc
		self.iters_post_amp = iters_post_amp
		self.ldpc_success = ldpc_success



#function to convert posteriorwise probabilities to bitwise probabilities
# note: my pp2bp takes the entire beta vector, whereas adams code works only on 1 section. Change this if neccessary
def pp2bp(β, L, M):
    #take in the section posterior probabilities β.
    #and the no. sections L and the size of each section M. 
    assert M%2 == 0
    
    #initialise numpy array of zeros for all of the bitwise posteriors
    p = np.zeros(int(log(M,2)*L))
    #loop through the L sections.
    for a in range(L):
        β_l = β[a*M:((a+1)*M)]
        c = np.sum(β_l) #calculate normalization constant for each section
        for logi in range(int(log(M,2))):
            b = int((a+1)*log(M,2) - logi - 1)
            i = pow(2, logi)
            k = i
            while k<M:
                #note shift in range of j due to βl being an array which is indexed from 0 not 1. 
                for j in range(k, k+i):
                    p[b] = p[b] + β_l[j]/c
                k = k + 2*i
    return p
    



#code has functions which convert bits2bytes, bytes2bits, bits2indices, indices2bits
#not sure if I'll actually need these. If I do someone has probably already written these in  python

# insert bits2bytes fn if needed 

# insert bytes2bits fn if needed 

# insert bits2indices fn if needed
def bits2indices(bits, m):
	assert m%2 == 0
	logm = int(log(m,2))
	assert len(bits)%logm == 0
	#finish this function


# insert indices2bits fn if needed 

#Fn to run point-to-point SPARC-AMP simulation with an LDPC outer code protecting the flat sections.
def lpdc_sim(ldpcparams: LDPCParams, seed)	-> LDPCResult:	
	#Get the LDPC parameters from the struct
	l = lpdcparms.l
	m = lpdcparams.m
	p = ldpcparams.p
	sigma2 = ldpcparams.sigma2
	r = ldpcparams.r
	r_pa = ldpcparams.r_pa
	t = lpdcparams.t


	#Pick LDPC code, get its parameters, (allocate memory?).
	#Can I just pick an LDPC code from python? Do I need to allocate memory when using python? Probs not
	# Use pyLDPC? Check with Ramji to see if he has any better suggestions. 


	#compute remaining parameters
	logm = int(log(m,2))
	n = int((l * logm - (nl-kl))/r) #nl and kl are the ldpc parameters n and k


	#set up random numbers generator
	np.random.seed(seed)

	#generate random user bits to transmit
	num_sparc_bits = l * logm
	num_user_bits = num_sparc_bits - (nl-kl)
	user_bits = np.random.randint(0, 2, num_user_bits).tolist()

	#split bits into amp bits and those that will be LDPC encoded
	#What are the AMP bits? Aren't all bits AMP decoded.
	amp_bits = user_bits[:(num_user_bits-kl)]
	ldpc_user_bits = user_bits[(num_user_bits-kl):]

	#LPDC encode those bits, via a mapping into bytes and back (this will determine if I need to bytes2bits fns etc.)

	#gather the AMP bits and the LDPC codeword bits (the "SPARC" bits)
	sparc_bits = amp_bits + ldpc_code_bits
	#turn all the SPARC bits into column choices
	#This is when I will need bits2indices
	sparc_indices = bits2indices(sparc_bits, m)


	#Make PA and FHT
	#PA is the power allocation? And FHT is the design matrix?
	#What are these? They are used for next comment. Is one the design matrix?

	#Encode sparc indecs into an actual codeword. 

	#generate channel noise and sum. 

	#Run AMP decoded

	#Gather the hard decision indices

	#Convert indices back to bits

	#Count bit errors in AMP alone. 

	#Convert all column probabilities into bit probabilities.

	#grab the LDPC bit probs, convert to LLRs, decode LDPC codeword

	#Count bit error in LDPC section

	#If LDPC worked, and there are errors in the unprotected bits, try running AMP again

		#Make a new beta_ldpc that only has the ldpc sections set

		#Compute a y that already has all the successfully decoded ldpc sections removed

		#Run AMP over the non-LDPC sections from scratch

		#Gather hard decision indices

	#Convert indices back to bits

	#Count bit errors in AMP unprotected bits

	#RETURN the LDPC results using the above defined struct


def amp_sim(L, M, σ_n, P, R, T, R_PA):
    # Compute the SNR, capacity, and n, from the input parameters
    snr = P / σ_n**2
    C = 0.5 * np.log2(1 + snr)
    n = int(L*np.log2(M) / R)
    
    # Generate the power allocation
    Pl = pa_iterative(L, L, σ_n, P, R_PA)
    
    # Generate random message in [0..M)^L
    tx_message = np.random.randint(0, M, L).tolist()
    
    # Generate the SPARC transform functions A.beta and A'.z
    Ab, Az = sparc_transforms(L, M, n)
    
    # Generate our transmitted signal X
    β_0 = np.zeros((L*M, 1))
    for l in range(L):
        β_0[l*M + tx_message[l]] = np.sqrt(n * Pl[l])
    x = Ab(β_0)
    
    # Generate random channel noise and then received signal y
    z = np.random.randn(n, 1) * σ_n
    y = (x + z).reshape(-1, 1)
        
    # Run AMP decoding
    β = amp(y, σ_n, Pl, L, M, T, Ab, Az).reshape(-1)
    #The above is the section wise probabilities. I need to convert these to bitwise probabilities. 
    
    
    print(bitwise_posterior(β, L, M))
    
    # Convert decoded beta back to a message
    rx_message = []
    for l in range(L):
        idx = np.argmax(β[l*M:(l+1)*M])
        rx_message.append(idx)
    
    # Compute fraction of sections decoded correctly
    correct = np.sum(np.array(rx_message) == np.array(tx_message)) / L
    
    # Compute BER note: np.sum will be deprecated soon, so look up alternative if this code stops working. 
    ber = np.sum(bin(a^b).count('1')
                 for (a, b) in zip(tx_message, rx_message))/(L*np.log2(M))
    
    # Compute Eb/N0
    EbN0 = 1/(2*R) * (P/σ_n**2)
    
    return {
        "L": L, "M": M, "sigma_n": σ_n, "P": P, "R": R, "T": T,
        "snr": snr, "C": C, "n": n, "fc": correct, "R_PA": R_PA,
        "ber": ber, "EbN0": EbN0, "ser": 1-correct
    }
